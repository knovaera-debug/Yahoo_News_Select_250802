import os
import requests
import gspread
from datetime import datetime
from bs4 import BeautifulSoup
from oauth2client.service_account import ServiceAccountCredentials

print("ğŸ“Œ å®Ÿè¡Œé–‹å§‹")

# --- èªè¨¼ï¼šGitHub Secrets ã® GOOGLE_CREDENTIALS ã‚’ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ ---
with open("tmp_creds.json", "w") as f:
    f.write(os.environ["GOOGLE_CREDENTIALS"])

scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
creds = ServiceAccountCredentials.from_json_keyfile_name('tmp_creds.json', scope)
client = gspread.authorize(creds)

# --- ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆIDå®šç¾© ---
KEYWORDS_SHEET_ID = "1yjHpQMHfJt7shjqZ6SYQNNlHougbrw0ZCgWpFUgv3Sc"
OUTPUT_SHEET_ID = "1ff9j8Dr2G6UO2GjsLNpgC8bW0KJmX994iJruw4X_qVM"

# --- ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å–å¾— ---
keywords_ws = client.open_by_key(KEYWORDS_SHEET_ID).sheet1
keywords = keywords_ws.col_values(1)[1:]  # Aåˆ—2è¡Œç›®ä»¥é™
print("âœ… ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸€è¦§:", keywords)

# --- å‡ºåŠ›ã‚·ãƒ¼ãƒˆã®æº–å‚™ ---
today_str = datetime.now().strftime("%y%m%d")
output_book = client.open_by_key(OUTPUT_SHEET_ID)

try:
    output_ws = output_book.worksheet(today_str)
    print(f"ğŸ“„ æ—¢å­˜ã‚·ãƒ¼ãƒˆ {today_str} ã‚’ä½¿ç”¨")
except gspread.exceptions.WorksheetNotFound:
    output_ws = output_book.add_worksheet(title=today_str, rows="1000", cols="20")
    print(f"ğŸ†• ã‚·ãƒ¼ãƒˆ {today_str} ã‚’æ–°è¦ä½œæˆ")

output_ws.clear()
output_ws.append_row(["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", "ã‚¿ã‚¤ãƒˆãƒ«", "URL", "æœ¬æ–‡ï¼ˆå†’é ­100å­—ï¼‰", "æ—¥ä»˜", "å–å¾—æ—¥æ™‚"])

# --- User-Agentå¼·åŒ–ï¼ˆBotå›é¿ï¼‰---
headers = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/115.0.0.0 Safari/537.36"
    )
}

# --- å„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«å¯¾ã—ã¦ Yahooãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢å®Ÿè¡Œ ---
for keyword in keywords:
    print(f"ğŸ” æ¤œç´¢é–‹å§‹: {keyword}")
    search_url = f"https://news.yahoo.co.jp/search?p={keyword}&ei=utf-8"

    try:
        res = requests.get(search_url, headers=headers, timeout=10)
        soup = BeautifulSoup(res.content, "html.parser")

        # æœ€æ–°ã®Yahooæ§‹é€ ã«å¯¾å¿œã—ãŸã‚»ãƒ¬ã‚¯ã‚¿
        articles = soup.select("div.sc-1out364-0")  # è¨˜äº‹å…¨ä½“ãƒ–ãƒ­ãƒƒã‚¯
        print(f"ã€€â†’ è¦‹ã¤ã‹ã£ãŸè¨˜äº‹æ•°: {len(articles)}")

        for article in articles[:5]:  # ä¸Šä½5ä»¶ã¾ã§å‡¦ç†
            try:
                title_tag = article.select_one("a")
                if not title_tag:
                    continue

                title = title_tag.text.strip()
                link = title_tag["href"]
                date_tag = article.select_one("time")
                date = date_tag.text.strip() if date_tag else ""

                # --- è¨˜äº‹æœ¬æ–‡ã®å–å¾— ---
                body = ""
                try:
                    detail = requests.get(link, headers=headers, timeout=10)
                    detail_soup = BeautifulSoup(detail.content, "html.parser")
                    tag = detail_soup.select_one("p.ynDetailText") or detail_soup.select_one(".article_body")
                    body = tag.text.strip() if tag else ""
                except Exception as e:
                    print(f"ã€€âš ï¸ æœ¬æ–‡å–å¾—å¤±æ•—: {e}")

                output_ws.append_row([
                    keyword,
                    title,
                    link,
                    body[:100],
                    date,
                    datetime.now().strftime("%Y/%m/%d %H:%M:%S")
                ])
            except Exception as e:
                print(f"ã€€âš ï¸ è¨˜äº‹å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ï¼ˆ{keyword}ï¼‰: {e}")
