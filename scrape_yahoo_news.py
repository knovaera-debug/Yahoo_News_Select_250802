import os
import json
import time
from datetime import datetime, timedelta
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from bs4 import BeautifulSoup
import gspread
from oauth2client.service_account import ServiceAccountCredentials

# --- è¨­å®š ---
KEYWORDS_SPREADSHEET_ID = '1yjHpQMHfJt7shjqZ6SYQNNlHougbrw0ZCgWpFUgv3Sc'
OUTPUT_SPREADSHEET_ID = '1ff9j8Dr2G6UO2GjsLNpgC8bW0KJmX994iJruw4X_qVM'
INPUT_SHEET_NAME = 'keywords'
HEADLESS = True

# --- Googleèªè¨¼ ---
google_creds = json.loads(os.environ["GOOGLE_CREDENTIALS"])
scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
credentials = ServiceAccountCredentials.from_json_keyfile_dict(google_creds, scope)
gc = gspread.authorize(credentials)

# --- ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰èª­ã¿è¾¼ã¿ ---
keyword_ws = gc.open_by_key(KEYWORDS_SPREADSHEET_ID).worksheet(INPUT_SHEET_NAME)
keywords = keyword_ws.col_values(1)[1:]  # A2ä»¥é™

# --- æ—¥ä»˜ã¨å‡ºåŠ›ã‚·ãƒ¼ãƒˆå ---
now = datetime.now()
JST = timedelta(hours=9)
today_jst = (now + JST).strftime("%y%m%d")
print("ğŸ“Œ å®Ÿè¡Œé–‹å§‹")
print("âœ… ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸€è¦§:", keywords)

# --- å‡ºåŠ›ç”¨ã‚·ãƒ¼ãƒˆæº–å‚™ ---
output_book = gc.open_by_key(OUTPUT_SPREADSHEET_ID)
try:
    output_ws = output_book.worksheet(today_jst)
except gspread.exceptions.WorksheetNotFound:
    base_ws = output_book.worksheet("Base")
    output_ws = base_ws.duplicate(new_sheet_name=today_jst)
print(f"ğŸ“„ æ—¢å­˜ã‚·ãƒ¼ãƒˆ {today_jst} ã‚’ä½¿ç”¨")

# --- Chromeãƒ‰ãƒ©ã‚¤ãƒè¨­å®šï¼ˆGitHub Actionså¯¾å¿œï¼‰ ---
options = Options()
if HEADLESS:
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')

driver = webdriver.Chrome(options=options)

# --- æ¤œç´¢å‡¦ç† ---
all_data = []
row_num = 2
for kw in keywords:
    print(f"ğŸ” æ¤œç´¢é–‹å§‹: {kw}")
    url = f"https://news.yahoo.co.jp/search?p={kw}&ei=utf-8"
    driver.get(url)
    time.sleep(2)

    soup = BeautifulSoup(driver.page_source, 'html.parser')
    articles = soup.select("li.sc-8tlg6o-0")  # Yahoo!è¨˜äº‹ãƒ–ãƒ­ãƒƒã‚¯ã‚»ãƒ¬ã‚¯ã‚¿
    print(f"ã€€â†’ è¨˜äº‹æ•°: {len(articles)}")

    for article in articles:
        try:
            title = article.select_one('a').text.strip()
            link = article.select_one('a')['href']
            date_elem = article.select_one("time")
            date_text = date_elem.text.strip() if date_elem else "ä¸æ˜"
            all_data.append([row_num - 1, kw, title, link, date_text])
            row_num += 1
        except Exception as e:
            print(f"âš ï¸ ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
            continue

# --- ãƒ‡ãƒ¼ã‚¿æ›¸ãè¾¼ã¿ï¼ˆãƒãƒ«ã‚¯ï¼‰---
if all_data:
    print(f"ğŸ“ å‡ºåŠ›è¡Œæ•°: {len(all_data)}")
    output_ws.update(f"A2:E{len(all_data)+1}", all_data)
else:
    print("ğŸ“­ æ›¸ãè¾¼ã‚€ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

driver.quit()
print("âœ… å‡¦ç†å®Œäº†")
