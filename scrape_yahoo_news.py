import os
import time
from datetime import datetime, timedelta
import pytz
import gspread
from oauth2client.service_account import ServiceAccountCredentials
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from bs4 import BeautifulSoup

# === Googleèªè¨¼è¨­å®š ===
scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
creds_path = "service_account.json"
credentials = ServiceAccountCredentials.from_json_keyfile_name(creds_path, scope)
gc = gspread.authorize(credentials)

# === ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆè¨­å®š ===
KEYWORDS_SPREADSHEET_ID = "1yjHpQMHfJt7shjqZ6SYQNNlHougbrw0ZCgWpFUgv3Sc"  # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç”¨
OUTPUT_SPREADSHEET_ID = "1ff9j8Dr2G6UO2GjsLNpgC8bW0KJmX994iJruw4X_qVM"  # å‡ºåŠ›å…ˆ

INPUT_SHEET_NAME = "keywords"
JST = pytz.timezone('Asia/Tokyo')
today_jst = datetime.now(JST).strftime("%y%m%d")

print("ğŸ“Œ å®Ÿè¡Œé–‹å§‹")

# === ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å–å¾— ===
keyword_ws = gc.open_by_key(KEYWORDS_SPREADSHEET_ID).worksheet(INPUT_SHEET_NAME)
keywords = [row[0] for row in keyword_ws.get_all_values()[1:] if row]
print(f"âœ… ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸€è¦§: {keywords}")

# === å‡ºåŠ›ã‚·ãƒ¼ãƒˆæº–å‚™ ===
output_book = gc.open_by_key(OUTPUT_SPREADSHEET_ID)
try:
    output_ws = output_book.worksheet(today_jst)
except gspread.exceptions.WorksheetNotFound:
    try:
        base_ws = output_book.worksheet("Base")
        output_ws = base_ws.duplicate(new_sheet_name=today_jst)
    except gspread.exceptions.WorksheetNotFound:
        output_ws = output_book.add_worksheet(title=today_jst, rows="100", cols="10")
        output_ws.update('A1:F1', [["No", "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", "ã‚¿ã‚¤ãƒˆãƒ«", "URL", "æ—¥ä»˜", "æœ¬æ–‡"]])

# === Chromeãƒ‰ãƒ©ã‚¤ãƒãƒ¼è¨­å®šï¼ˆãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ï¼‰ ===
options = Options()
options.add_argument('--headless')
options.add_argument('--disable-gpu')
options.add_argument('--no-sandbox')
driver = webdriver.Chrome(options=options)

row_idx = 2

# === å„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æ¤œç´¢å‡¦ç† ===
for kw in keywords:
    print(f"ğŸ” æ¤œç´¢é–‹å§‹: {kw}")
    url = f"https://news.yahoo.co.jp/search?p={kw}"
    driver.get(url)
    time.sleep(2)

    soup = BeautifulSoup(driver.page_source, "html.parser")
    articles = soup.select('div.sc-7n8lh-2.fvHeUD > div > article > div > div > div > a')

    print(f"ã€€â†’ è¨˜äº‹æ•°: {len(articles)}")

    for i, a_tag in enumerate(articles):
        try:
            href = a_tag.get("href")
            title = a_tag.get_text(strip=True)
            driver.get(href)
            time.sleep(1.5)
            article_soup = BeautifulSoup(driver.page_source, "html.parser")

            # æ—¥ä»˜å–å¾—
            date_tag = article_soup.select_one('time')
            date_text = date_tag.get_text(strip=True) if date_tag else ""

            # æœ¬æ–‡å–å¾—
            body_tag = article_soup.select_one("article")
            if not body_tag:
                body_tag = article_soup.select_one("div[class*='article']")
            body = body_tag.get_text(separator="\n", strip=True) if body_tag else ""

            # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«å‡ºåŠ›
            output_ws.update(f"A{row_idx}:F{row_idx}", [[i + 1, kw, title, href, date_text, body]])
            row_idx += 1
        except Exception as e:
            print(f"âš ï¸ å–å¾—å¤±æ•—: {e}")
            continue

driver.quit()
print("âœ… å®Œäº†")
