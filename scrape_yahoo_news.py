import os
import time
import gspread
import chromedriver_autoinstaller
from bs4 import BeautifulSoup
from datetime import datetime
from oauth2client.service_account import ServiceAccountCredentials
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By

print("ğŸ“Œ å®Ÿè¡Œé–‹å§‹")

# ChromeDriver è‡ªå‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
chromedriver_autoinstaller.install()

# Google èªè¨¼
with open("tmp_creds.json", "w") as f:
    f.write(os.environ["GOOGLE_CREDENTIALS"])

scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
creds = ServiceAccountCredentials.from_json_keyfile_name("tmp_creds.json", scope)
client = gspread.authorize(creds)

# ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ ID
KEYWORDS_SHEET_ID = "1yjHpQMHfJt7shjqZ6SYQNNlHougbrw0ZCgWpFUgv3Sc"
OUTPUT_SHEET_ID = "1ff9j8Dr2G6UO2GjsLNpgC8bW0KJmX994iJruw4X_qVM"

# ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å–å¾—
keywords_ws = client.open_by_key(KEYWORDS_SHEET_ID).sheet1
keywords = keywords_ws.col_values(1)[1:]
print("âœ… ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸€è¦§:", keywords)

# å‡ºåŠ›ç”¨ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆæº–å‚™
today_str = datetime.now().strftime("%y%m%d")
output_book = client.open_by_key(OUTPUT_SHEET_ID)

try:
    output_ws = output_book.worksheet(today_str)
    print(f"ğŸ“„ æ—¢å­˜ã‚·ãƒ¼ãƒˆ {today_str} ã‚’ä½¿ç”¨")
except:
    output_ws = output_book.add_worksheet(title=today_str, rows="1000", cols="20")
    print(f"ğŸ†• ã‚·ãƒ¼ãƒˆ {today_str} ã‚’æ–°è¦ä½œæˆ")

output_ws.clear()
output_ws.append_row(["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", "ã‚¿ã‚¤ãƒˆãƒ«", "URL", "æœ¬æ–‡ï¼ˆå†’é ­100å­—ï¼‰", "æ—¥ä»˜", "å–å¾—æ—¥æ™‚"])

# Selenium ãƒ–ãƒ©ã‚¦ã‚¶è¨­å®š
options = Options()
options.add_argument('--headless')
options.add_argument('--no-sandbox')
options.add_argument('--disable-dev-shm-usage')
driver = webdriver.Chrome(options=options)

# å„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æ¤œç´¢ãƒ»åé›†
for keyword in keywords:
    print(f"ğŸ” æ¤œç´¢é–‹å§‹: {keyword}")
    search_url = f"https://news.yahoo.co.jp/search?p={keyword}&ei=utf-8"
    driver.get(search_url)
    time.sleep(3)  # JavaScriptãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°å¾…ã¡

    soup = BeautifulSoup(driver.page_source, "html.parser")
    articles = soup.select("div.sc-1out364-0")
    print(f"ã€€â†’ è¨˜äº‹æ•°: {len(articles)}")

    for article in articles[:5]:
        try:
            a_tag = article.select_one("a")
            if not a_tag:
                continue

            title = a_tag.text.strip()
            link = a_tag["href"]
            date_tag = article.select_one("time")
            date = date_tag.text if date_tag else ""

            # æœ¬æ–‡å–å¾—
            driver.get(link)
            time.sleep(2)
            detail_soup = BeautifulSoup(driver.page_source, "html.parser")
            tag = detail_soup.select_one("p.ynDetailText") or detail_soup.select_one(".article_body")
            body = tag.text.strip() if tag else ""

            output_ws.append_row([
                keyword,
                title,
                link,
                body[:100],
                date,
                datetime.now().strftime("%Y/%m/%d %H:%M:%S")
            ])
        except Exception as e:
            print(f"âš ï¸ è¨˜äº‹å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")

driver.quit()
