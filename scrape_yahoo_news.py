import chromedriver_autoinstaller
chromedriver_autoinstaller.install()

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import gspread
from oauth2client.service_account import ServiceAccountCredentials
import json
import os
import time

# âœ… Googleèªè¨¼æƒ…å ±ã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰èª­ã¿è¾¼ã‚€
credentials_json = os.getenv('GOOGLE_CREDENTIALS')
if credentials_json is None:
    raise ValueError("ç’°å¢ƒå¤‰æ•° GOOGLE_CREDENTIALS ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

credentials_dict = json.loads(credentials_json)
scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
credentials = ServiceAccountCredentials.from_json_keyfile_dict(credentials_dict, scope)
gc = gspread.authorize(credentials)

# âœ… Chromeè¨­å®š
options = Options()
options.add_argument('--headless')
options.add_argument('--no-sandbox')
options.add_argument('--disable-dev_shm_usage')
driver = webdriver.Chrome(options=options)

try:
    # âœ… å…¥åŠ›ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‹ã‚‰URLã‚’ã™ã¹ã¦å–å¾—
    INPUT_SPREADSHEET_ID = '1yjHpQMHfJt7shjqZ6SYQNNlHougbrw0ZCgWpFUgv3Sc'
    input_ws = gc.open_by_key(INPUT_SPREADSHEET_ID).worksheet('URLS')
    # A2ä»¥é™ã®Aåˆ—ã®å€¤ã‚’ã™ã¹ã¦å–å¾—
    urls = input_ws.col_values(1)[1:]

    # âœ… å‡ºåŠ›ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚’é–‹ã
    OUTPUT_SPREADSHEET_ID = '1ff9j8Dr2G6UO2GjsLNpgC8bW0KJmX994iJruw4X_qVM'
    output_ws = gc.open_by_key(OUTPUT_SPREADSHEET_ID).worksheet('Base')

    for base_url in urls:
        if not base_url:
            continue
            
        # æœ€åˆã®ãƒšãƒ¼ã‚¸ï¼ˆpage=1ï¼‰ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ã‚¿ã‚¤ãƒˆãƒ«ã¨æŠ•ç¨¿æ—¥æ™‚ã‚’å–å¾—
        try:
            driver.get(base_url)
            time.sleep(3) # ãƒšãƒ¼ã‚¸ã®æç”»ã‚’å¾…ã¤ãŸã‚ã«ä¸€æ™‚åœæ­¢ã‚’è¿½åŠ 
            initial_soup = BeautifulSoup(driver.page_source, 'html.parser')
            
            # âœ… B3ã«ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ›¸ãè¾¼ã¿ (titleã‚¿ã‚°ã‹ã‚‰å–å¾—ã—ã€ä¸è¦ãªéƒ¨åˆ†ã‚’å‰Šé™¤)
            page_title = initial_soup.title.string if initial_soup.title else 'å–å¾—ä¸å¯'
            news_title = page_title.replace(' - Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹', '').strip() if ' - Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹' in page_title else page_title.strip()
            output_ws.update('B3', [[news_title]])
            print(f"âœ… B3ã‚»ãƒ«ã«ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ›¸ãè¾¼ã¿ã¾ã—ãŸ: {news_title}")

            # âœ… B4ã«URLã‚’æ›¸ãè¾¼ã¿
            output_ws.update('B4', [[base_url]])
            print(f"âœ… B4ã‚»ãƒ«ã«URLã‚’æ›¸ãè¾¼ã¿ã¾ã—ãŸ: {base_url}")

            # âœ… B5ã«æŠ•ç¨¿æ—¥æ™‚ã‚’æ›¸ãè¾¼ã¿
            date_tag = initial_soup.find('time')
            news_date = date_tag.text.strip() if date_tag else 'å–å¾—ä¸å¯'
            output_ws.update('B5', [[news_date]])
            print(f"âœ… B5ã‚»ãƒ«ã«æŠ•ç¨¿æ—¥æ™‚ã‚’æ›¸ãè¾¼ã¿ã¾ã—ãŸ: {news_date}")
            
        except Exception as e:
            print(f"âš ï¸ ã‚¿ã‚¤ãƒˆãƒ«ã€URLã€æŠ•ç¨¿æ—¥æ™‚ã®å–å¾—ã¾ãŸã¯æ›¸ãè¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            
        # è¨˜äº‹æœ¬æ–‡ï¼ˆè¤‡æ•°ãƒšãƒ¼ã‚¸å¯¾å¿œï¼‰ã®å–å¾—ã¨æ›¸ãè¾¼ã¿
        page_number = 1
        while True:
            if page_number == 1:
                article_url = base_url
            else:
                article_url = f"{base_url}?page={page_number}"

            print(f"ğŸ” URL: {article_url} ã®è¨˜äº‹æœ¬æ–‡ã‚’å–å¾—ã—ã¾ã™ã€‚")

            article_body = ""
            driver.get(article_url)

            if "æŒ‡å®šã•ã‚ŒãŸURLã¯å­˜åœ¨ã—ã¾ã›ã‚“ã§ã—ãŸã€‚" in driver.page_source:
                print(f"â„¹ï¸ {page_number}ãƒšãƒ¼ã‚¸ç›®ã¯å­˜åœ¨ã—ã¾ã›ã‚“ã§ã—ãŸã€‚æœ¬æ–‡ã®å–å¾—ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
                break

            try:
                WebDriverWait(driver, 30).until(
                    EC.presence_of_element_located((By.TAG_NAME, 'article'))
                )
                article_soup = BeautifulSoup(driver.page_source, 'html.parser')
                article_container = article_soup.find('article')
                if article_container:
                    article_body = article_container.get_text(separator='\n', strip=True)
                    print(f"âœ… {page_number}ãƒšãƒ¼ã‚¸ç›®ã®è¨˜äº‹æœ¬æ–‡ã®å–å¾—ã«æˆåŠŸã—ã¾ã—ãŸã€‚")
                    print(f"ã€€å–å¾—ã—ãŸæœ¬æ–‡ã®æ–‡å­—æ•°: {len(article_body)}æ–‡å­—")
                    print(f"ã€€å–å¾—ã—ãŸæœ¬æ–‡ã®å†’é ­: {article_body[:50]}...")
                else:
                    raise NoSuchElementException(f"{page_number}ãƒšãƒ¼ã‚¸ç›®ã®è¨˜äº‹æœ¬æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            except (TimeoutException, NoSuchElementException) as e:
                print(f"âš ï¸ {page_number}ãƒšãƒ¼ã‚¸ç›®ã®è¨˜äº‹æœ¬æ–‡ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                print("--- ãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼ˆHTMLã‚½ãƒ¼ã‚¹ã®å†’é ­500æ–‡å­—ï¼‰ ---")
                print(driver.page_source[:500])
                print("-------------------------------------------------")
                article_body = "è¨˜äº‹æœ¬æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"

            try:
                row_to_write = f'B{5 + page_number}'

                if len(article_body) > 50000:
                    truncated_body = article_body[:50000] + "..."
                    output_ws.update(row_to_write, [[truncated_body]])
                    print(f"âœ… è¨˜äº‹æœ¬æ–‡ãŒé•·ã„ãŸã‚ã€50000æ–‡å­—ã«åˆ¶é™ã—ã¦{row_to_write}ã‚»ãƒ«ã«æ›¸ãè¾¼ã¿ã¾ã—ãŸã€‚")
                else:
                    output_ws.update(row_to_write, [[article_body]])
                    print(f"âœ… {row_to_write}ã‚»ãƒ«ã«è¨˜äº‹æœ¬æ–‡ã‚’æ›¸ãè¾¼ã¿ã¾ã—ãŸã€‚")
            except Exception as e:
                print(f"âš ï¸ æ›¸ãè¾¼ã¿å¤±æ•—: {e}")
            
            page_number += 1

        # âœ… ã‚³ãƒ¡ãƒ³ãƒˆã®å–å¾—ã¨æ›¸ãè¾¼ã¿
        print("-" * 20)
        print("ğŸ” ã‚³ãƒ¡ãƒ³ãƒˆã®å–å¾—ã‚’é–‹å§‹ã—ã¾ã™ã€‚")
        all_comments = []
        comment_page_number = 1
        while True:
            # ã‚³ãƒ¡ãƒ³ãƒˆãƒšãƒ¼ã‚¸ã®URLã‚’æ§‹ç¯‰
            if comment_page_number == 1:
                comment_url = f"{base_url}/comments"
            else:
                comment_url = f"{base_url}/comments?page={comment_page_number}"

            print(f"ğŸ” URL: {comment_url} ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’å–å¾—ã—ã¾ã™ã€‚")
            driver.get(comment_url)
            
            # ãƒšãƒ¼ã‚¸ãŒå­˜åœ¨ã—ãªã„å ´åˆã®ãƒã‚§ãƒƒã‚¯
            if "æŒ‡å®šã•ã‚ŒãŸURLã¯å­˜åœ¨ã—ã¾ã›ã‚“ã§ã—ãŸã€‚" in driver.page_source:
                print(f"â„¹ï¸ ã‚³ãƒ¡ãƒ³ãƒˆã®{comment_page_number}ãƒšãƒ¼ã‚¸ç›®ã¯å­˜åœ¨ã—ã¾ã›ã‚“ã§ã—ãŸã€‚ã‚³ãƒ¡ãƒ³ãƒˆã®å–å¾—ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
                break
            
            try:
                # ã‚³ãƒ¡ãƒ³ãƒˆã®ã‚³ãƒ³ãƒ†ãƒŠãŒèª­ã¿è¾¼ã¾ã‚Œã‚‹ã¾ã§å¾…æ©Ÿ
                WebDriverWait(driver, 30).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, '.ca-list-item'))
                )
                comment_soup = BeautifulSoup(driver.page_source, 'html.parser')
                # å„ã‚³ãƒ¡ãƒ³ãƒˆã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
                comments_on_page = comment_soup.select('.ca-list-item')
                if comments_on_page:
                    for comment_item in comments_on_page:
                        comment_text = comment_item.find('p', class_='ca-body').text.strip()
                        all_comments.append(comment_text)
                    print(f"âœ… ã‚³ãƒ¡ãƒ³ãƒˆã®{comment_page_number}ãƒšãƒ¼ã‚¸ç›®ã‹ã‚‰{len(comments_on_page)}ä»¶å–å¾—ã—ã¾ã—ãŸã€‚")
                else:
                    print(f"â„¹ï¸ ã‚³ãƒ¡ãƒ³ãƒˆã®{comment_page_number}ãƒšãƒ¼ã‚¸ç›®ã«ã‚³ãƒ¡ãƒ³ãƒˆã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                    break

            except (TimeoutException, NoSuchElementException) as e:
                print(f"âš ï¸ ã‚³ãƒ¡ãƒ³ãƒˆã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                break

            comment_page_number += 1

        # âœ… å–å¾—ã—ãŸã‚³ãƒ¡ãƒ³ãƒˆã®ç·æ•°ã‚’B18ã«æ›¸ãè¾¼ã¿
        try:
            output_ws.update('B18', [[len(all_comments)]])
            print(f"âœ… B18ã‚»ãƒ«ã«ã‚³ãƒ¡ãƒ³ãƒˆç·æ•°ï¼ˆ{len(all_comments)}ä»¶ï¼‰ã‚’æ›¸ãè¾¼ã¿ã¾ã—ãŸã€‚")
        except Exception as e:
            print(f"âš ï¸ ã‚³ãƒ¡ãƒ³ãƒˆç·æ•°ã®æ›¸ãè¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

        # âœ… å–å¾—ã—ãŸã‚³ãƒ¡ãƒ³ãƒˆã‚’B19ä»¥é™ã«æ›¸ãè¾¼ã¿
        if all_comments:
            try:
                # ã‚³ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆã‚’æ›¸ãè¾¼ã¿ç”¨ã«å¤‰æ›
                comments_to_write = [[c] for c in all_comments]
                output_ws.update('B19', comments_to_write)
                print(f"âœ… B19ã‚»ãƒ«ä»¥é™ã«{len(all_comments)}ä»¶ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’æ›¸ãè¾¼ã¿ã¾ã—ãŸã€‚")
            except Exception as e:
                print(f"âš ï¸ ã‚³ãƒ¡ãƒ³ãƒˆã®æ›¸ãè¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        else:
            print("â„¹ï¸ å–å¾—ã—ãŸã‚³ãƒ¡ãƒ³ãƒˆã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
finally:
    driver.quit()
    print("âœ… å®Œäº†")
