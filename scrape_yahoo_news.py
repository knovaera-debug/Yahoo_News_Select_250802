from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from bs4 import BeautifulSoup
import time
from datetime import datetime
import gspread
import json
import os

# Google Sheets èªè¨¼
credentials = json.loads(os.environ["GOOGLE_CREDENTIALS"])
gc = gspread.service_account_from_dict(credentials)

# ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆè¨­å®š
KEYWORDS_SPREADSHEET_ID = "1yjHpQMHfJt7shjqZ6SYQNNlHougbrw0ZCgWpFUgv3Sc"
OUTPUT_SPREADSHEET_ID = "1ff9j8Dr2G6UO2GjsLNpgC8bW0KJmX994iJruw4X_qVM"
INPUT_SHEET_NAME = "keywords"
TODAY_SHEET_NAME = datetime.now().strftime("%y%m%d")

# ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å–å¾—
keyword_ws = gc.open_by_key(KEYWORDS_SPREADSHEET_ID).worksheet(INPUT_SHEET_NAME)
keywords = keyword_ws.col_values(1)[1:]  # A2ä»¥é™
print(f"ğŸ“Œ å®Ÿè¡Œé–‹å§‹")
print(f"âœ… ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸€è¦§: {keywords}")

# å‡ºåŠ›å…ˆã‚·ãƒ¼ãƒˆ
sh = gc.open_by_key(OUTPUT_SPREADSHEET_ID)
try:
    output_ws = sh.worksheet(TODAY_SHEET_NAME)
    print(f"ğŸ“„ æ—¢å­˜ã‚·ãƒ¼ãƒˆ {TODAY_SHEET_NAME} ã‚’ä½¿ç”¨")
except:
    output_ws = sh.add_worksheet(title=TODAY_SHEET_NAME, rows="1000", cols="10")
    output_ws.append_row(["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", "è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«", "è¨˜äº‹URL", "æœ¬æ–‡å†’é ­", "è¨˜äº‹æ—¥ä»˜", "å–å¾—æ—¥æ™‚"])
    print(f"ğŸ†• æ–°è¦ã‚·ãƒ¼ãƒˆ {TODAY_SHEET_NAME} ã‚’ä½œæˆ")

# Chromeãƒ‰ãƒ©ã‚¤ãƒè¨­å®š
options = Options()
options.add_argument('--headless')
options.add_argument('--disable-gpu')
options.add_argument('--no-sandbox')
options.add_argument('--window-size=1280x800')
driver = webdriver.Chrome(options=options)

for keyword in keywords:
    print(f"ğŸ” æ¤œç´¢é–‹å§‹: {keyword}")
    query_url = f"https://news.yahoo.co.jp/search?p={keyword}&ei=utf-8"
    driver.get(query_url)
    time.sleep(3)

    soup = BeautifulSoup(driver.page_source, "html.parser")
    print("ğŸŒ ãƒšãƒ¼ã‚¸ã‚½ãƒ¼ã‚¹å†’é ­ï¼ˆ1000æ–‡å­—ï¼‰:")
    print(driver.page_source[:1000])

    articles = soup.select("a[href^='https://news.yahoo.co.jp/articles/']")
    print(f"ã€€â†’ è¨˜äº‹æ•°: {len(articles)}")

    for article in articles[:5]:
        try:
            title = article.text.strip()
            link = article["href"]
            date = ""  # è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰å–å¾—ã—ãªã„é™ã‚Šç©º

            # æœ¬æ–‡å–å¾—
            driver.get(link)
            time.sleep(2)
            detail_soup = BeautifulSoup(driver.page_source, "html.parser")
            tag = detail_soup.select_one("p.ynDetailText") or detail_soup.select_one(".article_body") or detail_soup.select_one("div.article_body")
            body = tag.text.strip() if tag else ""

            output_ws.append_row([
                keyword,
                title,
                link,
                body[:100],
                date,
                datetime.now().strftime("%Y/%m/%d %H:%M:%S")
            ])
        except Exception as e:
            print(f"âš ï¸ è¨˜äº‹å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")

driver.quit()
print("âœ… å®Œäº†")
